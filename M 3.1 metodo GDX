{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPpDKwQdtrHolaUo+1uuNmt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"K07X9pOi5sO-"},"outputs":[],"source":["import numpy as np\n","\n","# objective Extended Rosenbrock function\n","def objfcn(x):\n","    # Minima -> f=0 at (1,.....,1)\n","    n = len(x) # n even\n","    fvec = np.zeros((n,1))\n","    idx1 = np.array(range(0,n,2)) # odd index\n","    idx2 = np.array(range(1,n,2)) # even index\n","    fvec[idx1]=10.0*(x[idx2]-(x[idx1])**2.0)\n","    fvec[idx2]=1.0-x[idx1]\n","    f = fvec.T @ fvec\n","    return f[0,0]\n","\n","# Extended Rosenbrock gradient function\n","def objfcngrad(x):\n","    n = len(x) # n even\n","    Jf = np.zeros((n,n))\n","    fvec = np.zeros((n,1))\n","    idx1 = np.array(range(0,n,2)) # odd index\n","    idx2 = np.array(range(1,n,2)) # even index\n","    fvec[idx1]=10.0*(x[idx2]-(x[idx1])**2.0)\n","    fvec[idx2]=1.0-x[idx1]\n","\n","    for i in range(n//2):\n","        Jf[2*i,2*i]     = -20.0*x[2*i]\n","        Jf[2*i,2*i+1]   = 10.0\n","        Jf[2*i+1,2*i]   = -1.0\n","\n","    gX = 2.0*Jf.T @ fvec\n","    return gX\n","\n","def objfcnjac(x):\n","    n = len(x) # n even\n","    Jf = np.zeros((n,n))\n","    fvec = np.zeros((n,1))\n","    idx1 = np.array(range(0,n,2)) # odd index\n","    idx2 = np.array(range(1,n,2)) # even index\n","    fvec[idx1]=10.0*(x[idx2]-(x[idx1])**2.0)\n","    fvec[idx2]=1.0-x[idx1]\n","\n","    for i in range(n//2):\n","        Jf[2*i,2*i]     = -20.0*x[2*i]\n","        Jf[2*i,2*i+1]   = 10.0\n","        Jf[2*i+1,2*i]   = -1.0\n","\n","    gX = 2.0*Jf.T @ fvec\n","    normgX = np.linalg.norm(gX)\n","    return fvec, Jf, normgX\n","\n","def traingdx(X,maxEpochs,goal,lr,mc,lr_inc,lr_dec,max_perf_inc,mingrad,show):\n","    this = \"traingdx\"\n","    stop = \"\"\n","    epochs = []\n","    perfs  = []\n","    dX = 0\n","    for epoch in range(maxEpochs+1):\n","        perf = objfcn(X)\n","        gX = objfcngrad(X)\n","        normgX = np.linalg.norm(gX)\n","        if perf <= goal:\n","            stop = \"Performance goal met\"\n","        elif epoch == maxEpochs:\n","            stop = \"Maximum epoch reached, performance goal was not met\"\n","        elif normgX < mingrad:\n","            stop = \"Minimum gradient reached, performance goal was not met\"\n","        # Progress\n","        if (np.fmod(epoch,show) == 0 or len(stop) != 0):\n","            print(this,end = \": \")\n","            if np.isfinite(maxEpochs):\n","                print(\"Epoch \",epoch, \"/\", maxEpochs,end = \" \")\n","            if np.isfinite(goal):\n","                print(\", Performance %8.3e\" % perf, \"/\", goal, end = \" \")\n","            if np.isfinite(mingrad):\n","                print(\", Gradient %8.3e\" % normgX, \"/\", mingrad)\n","\n","            epochs = np.append(epochs,epoch)\n","            perfs = np.append(perfs,perf)\n","            if len(stop) != 0:\n","                print(\"\\n\",this,\":\",stop,\"\\n\")\n","                break\n","        dX = mc*dX - (1-mc)*lr*gX\n","        X2 = X + dX\n","        perf2 = objfcn(X2)\n","\n","        if (perf2/perf) > max_perf_inc :\n","            lr = lr*lr_dec\n","            dX = lr*gX\n","        else:\n","            if (perf2 < perf):\n","                lr = lr*lr_inc\n","            X = X2\n","            perf = perf2\n","            gX   = objfcngrad(X)\n","            normgX = np.linalg.norm(gX)\n","\n","    return X, perfs, epochs\n","\n","#def main():\n","# seed the pseudo random number generator\n","np.random.seed(1)\n","X = 2*(np.random.randn(10,1))\n","goal = 1e-13\n","# define the total iterations\n","max_epochs = 300000\n","# rate learning\n","lr = 1e-4\n","# momentum\n","mc = 0.1\n","# rate learning increment\n","lr_inc = 1.05\n","# rate learning decrement\n","lr_dec = 0.70\n","# maximum performance increment\n","max_perf_inc = 1.04\n","# minimum gradient\n","min_grad = 1e-11\n","# show\n","show = 5000\n","# perform the gradient descent\n","X, perfs, epochs = traingdx(X,max_epochs,goal,lr,mc,lr_inc,lr_dec,max_perf_inc,min_grad,show)\n","print(X)"]},{"cell_type":"code","source":[],"metadata":{"id":"W7J7wv_B9mj7"},"execution_count":null,"outputs":[]}]}